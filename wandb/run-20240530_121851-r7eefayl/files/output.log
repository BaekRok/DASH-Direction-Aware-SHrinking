Files already downloaded and verified
Files already downloaded and verified
ConvNet(
  (conv1): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1))
  (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU()
  (conv_layers): Sequential(
    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1))
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2))
    (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1))
    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2))
    (10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU()
    (12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1))
    (13): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): ReLU()
    (15): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2))
    (16): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU()
  )
  (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1))
  (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu2): ReLU()
  (final_conv1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
  (final_bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu_final1): ReLU()
  (final_conv2): Conv2d(768, 10, kernel_size=(1, 1), stride=(1, 1))
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
)
Epoch [1/140], Loss: 1.5358, Train Accuracy: 0.4402, Test Accuracy: 0.5367
Epoch [2/140], Loss: 1.0699, Train Accuracy: 0.6181, Test Accuracy: 0.5781
Epoch [3/140], Loss: 0.7863, Train Accuracy: 0.7226, Test Accuracy: 0.6870
Epoch [4/140], Loss: 0.5621, Train Accuracy: 0.8036, Test Accuracy: 0.6696
Epoch [5/140], Loss: 0.3986, Train Accuracy: 0.8602, Test Accuracy: 0.6915
Epoch [6/140], Loss: 0.2429, Train Accuracy: 0.9178, Test Accuracy: 0.6989
Epoch [7/140], Loss: 0.1739, Train Accuracy: 0.9399, Test Accuracy: 0.6930
Epoch [8/140], Loss: 0.1309, Train Accuracy: 0.9549, Test Accuracy: 0.6880
Epoch [9/140], Loss: 0.1059, Train Accuracy: 0.9623, Test Accuracy: 0.7045
Epoch [10/140], Loss: 0.0763, Train Accuracy: 0.9731, Test Accuracy: 0.7163
Epoch [11/140], Loss: 0.0425, Train Accuracy: 0.9863, Test Accuracy: 0.7250
Epoch [12/140], Loss: 0.0304, Train Accuracy: 0.9905, Test Accuracy: 0.7370
Epoch [13/140], Loss: 0.0117, Train Accuracy: 0.9972, Test Accuracy: 0.7374
Epoch [14/140], Loss: 0.0079, Train Accuracy: 0.9977, Test Accuracy: 0.7369
Traceback (most recent call last):
  File "/home/baekrok/Plasticity/temp2.py", line 139, in <module>
    train(args)
  File "/home/baekrok/Plasticity/temp2.py", line 94, in train
    for inputs, labels in chunk_loader:
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 364, in __getitems__
    return [self.dataset[self.indices[idx]] for idx in indices]
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 364, in <listcomp>
    return [self.dataset[self.indices[idx]] for idx in indices]
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/datasets/cifar.py", line 118, in __getitem__
    img = self.transform(img)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 277, in forward
    return F.normalize(tensor, self.mean, self.std, self.inplace)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/transforms/functional.py", line 363, in normalize
    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)
  File "/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py", line 927, in normalize
    std = std.view(-1, 1, 1)
KeyboardInterrupt